{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jg4xlzr_b1dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "id": "veHkFUA0wy1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "arSaj9CYUOoJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing (from previous [tutorial](https://substack.com/inbox/post/135885628))"
      ],
      "metadata": {
        "id": "K9aY8Wmnb4sZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtmNjv6IUVio"
      },
      "outputs": [],
      "source": [
        "# download tiny_shakespeare dataset\n",
        "dataset_dict = tfds.load(name='tiny_shakespeare')\n",
        "\n",
        "# get train/validation/test data\n",
        "train_data = dataset_dict['train']\n",
        "validation_data = dataset_dict['validation']\n",
        "test_data = dataset_dict['test']\n",
        "\n",
        "'''\n",
        "each dataset contains 1 example of string type\n",
        "split the string into a sequence of Unicode code points\n",
        "'''\n",
        "train_dataset = train_data.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
        "validation_dataset = validation_data.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
        "test_dataset = test_data.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
        "\n",
        "vocabulary = sorted(set(next(iter(train_dataset)).numpy()))\n",
        "\n",
        "ids_to_tokens = {id:token for id, token in enumerate(vocabulary)}\n",
        "tokens_to_ids = {token:id for id, token in enumerate(vocabulary)}\n",
        "\n",
        "keys_tensor = tf.constant(list(tokens_to_ids.keys()))\n",
        "vals_tensor = tf.constant(list(tokens_to_ids.values()))\n",
        "tokens_to_ids_loopup = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=-1\n",
        ")\n",
        "\n",
        "def decode(line):\n",
        "  return ''.join(ids_to_tokens[id].decode('UTF-8') for id in line)\n",
        "\n",
        "def tokenize(line):\n",
        "  return tokens_to_ids_loopup.lookup(line)\n",
        "\n",
        "# configuration to store model parameters\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"vocab_size\": len(vocabulary),\n",
        "    'batch_size': 32,\n",
        "    'context_window': 16,\n",
        "    'd_model': 128,\n",
        "    'epochs': 10,\n",
        "    'n_heads': 8,\n",
        "    'n_layers': 4,\n",
        "}\n",
        "\n",
        "# tokenize and create sequence\n",
        "def process_dataset(dataset, model_config, is_train=False):\n",
        "  dataset = dataset.map(lambda x: tokenize(x))\n",
        "  # shift the sequence by 1\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[1:]))\n",
        "  dataset = dataset.unbatch()\n",
        "  dataset = dataset.batch(model_config['context_window'], drop_remainder=True)\n",
        "  dataset = dataset.batch(model_config['batch_size'], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "processed_train_dataset = process_dataset(train_dataset, MODEL_CONFIG, True)\n",
        "processed_validation_dataset = process_dataset(validation_dataset, MODEL_CONFIG)\n",
        "processed_test_dataset = process_dataset(test_dataset, MODEL_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFUN6ERwRdgM"
      },
      "outputs": [],
      "source": [
        "# [OPTIONAL] store training results in google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_dir = '/content/drive/MyDrive/Colab Notebooks/llama'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BabyLLaMA Model (from previous [tutorial](https://substack.com/inbox/post/135885628))"
      ],
      "metadata": {
        "id": "whLqzdDAQW7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model return logits rather than normalized probabilities\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "\n",
        "def generate_text(model, config, sentence_count=5, max_new_tokens=50):\n",
        "    idx = tf.zeros([sentence_count, 1], dtype=tf.int64)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits  = model(idx[:, -config['context_window']:])\n",
        "        # get the distribution of the last token\n",
        "        p = logits[:, -1,:]\n",
        "        # use the distribution p to sample the next token\n",
        "        idx_next = tf.random.categorical(p, num_samples=1, dtype=tf.int64)\n",
        "        idx = tf.concat([idx, idx_next], axis=-1)\n",
        "    return [decode(line) for line in idx.numpy()]"
      ],
      "metadata": {
        "id": "eNHepr8jZuqs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSNormLayer\n",
        "\n",
        "added `get_prunable_weights` for future pruning"
      ],
      "metadata": {
        "id": "IbRfIo_XxzSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNormLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "    super().__init__()\n",
        "    self.scale = self.add_weight(\"scale\", shape=layer_shape, initializer=\"ones\", trainable=True)\n",
        "    self.eps = eps\n",
        "    if bias:\n",
        "      self.bias = self.add_weight(\"bias\", shape=layer_shape, initializer=\"zeros\", trainable=True)\n",
        "    else:\n",
        "      self.bias = None\n",
        "\n",
        "  def get_prunable_weights(self):\n",
        "    return self.trainable_weights\n",
        "\n",
        "  def call(self, x):\n",
        "    # Frobenius norm\n",
        "    fro_norm = tf.norm(x, ord='fro', axis=[1, 2]) * tf.math.pow(tf.cast(tf.reduce_prod(tf.shape(x[0])), tf.float32), -0.5)\n",
        "    normalized = x / (tf.expand_dims(tf.expand_dims(fro_norm, -1), -1) + self.eps)\n",
        "    scaled = tf.expand_dims(self.scale[:x.shape[1], :], 0) * normalized\n",
        "\n",
        "    if self.bias is not None:\n",
        "      return scaled + self.bias\n",
        "    else:\n",
        "      return scaled"
      ],
      "metadata": {
        "id": "6B7zqk765449"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoPE\n",
        "added `get_prunable_weights` for future pruning"
      ],
      "metadata": {
        "id": "93Lz6Q3TQeYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Q_PubLCWxIop"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "  mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask\n",
        "\n",
        "class RoPEAttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.w_q = tf.keras.layers.Dense(config['d_model'], use_bias=False)\n",
        "    self.w_k = tf.keras.layers.Dense(config['d_model'], use_bias=False)\n",
        "    self.w_v = tf.keras.layers.Dense(config['d_model'], use_bias=False)\n",
        "\n",
        "    self.multihead = tf.keras.layers.MultiHeadAttention(config['n_heads'], config['d_model'], dropout=0.1)\n",
        "\n",
        "    self.rotary_matrix = self.get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "  def get_prunable_weights(self):\n",
        "    return self.trainable_weights\n",
        "\n",
        "  def get_rotary_matrix(self, context_window, embedding_dim):\n",
        "    matrix = tf.zeros((context_window, embedding_dim, embedding_dim))\n",
        "\n",
        "    for position in range(context_window):\n",
        "      for i in range(embedding_dim // 2):\n",
        "        theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
        "        m_theta = position * theta\n",
        "\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i, 2 * i]], [tf.math.cos(m_theta)])\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i, 2 * i + 1]], [-tf.math.sin(m_theta)])\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i + 1, 2 * i]], [tf.math.sin(m_theta)])\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i + 1, 2 * i + 1]], [tf.math.cos(m_theta)])\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  def call(self, x):\n",
        "    _, context_window, _ = x.shape\n",
        "    q = self.w_q(x)\n",
        "    k = self.w_k(x)\n",
        "    v = self.w_v(x)\n",
        "    q_out = tf.linalg.matmul(tf.transpose(q, perm=[1, 0, 2]), self.rotary_matrix[:context_window, ...])\n",
        "    q_out = tf.transpose(q_out, perm=[1, 0, 2])\n",
        "    k_out = tf.linalg.matmul(tf.transpose(k, perm=[1, 0, 2]), self.rotary_matrix[:context_window, ...])\n",
        "    k_out = tf.transpose(k_out, perm=[1, 0, 2])\n",
        "    v_out = tf.linalg.matmul(tf.transpose(v, perm=[1, 0, 2]), self.rotary_matrix[:context_window, ...])\n",
        "    v_out = tf.transpose(v_out, perm=[1, 0, 2])\n",
        "\n",
        "    activations = self.multihead(\n",
        "        q_out, k_out, v_out,\n",
        "        attention_mask=generate_square_subsequent_mask(context_window),\n",
        "        return_attention_scores=False,\n",
        "        use_causal_mask=True\n",
        "    )\n",
        "    return activations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SwiGLU\n",
        "added `get_prunable_weights` for future pruning"
      ],
      "metadata": {
        "id": "IXYb286OQiqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fwmFcNYiafck"
      },
      "outputs": [],
      "source": [
        "class SwiGLU(tf.keras.layers.Layer):\n",
        "  def __init__(self, size):\n",
        "    super().__init__()\n",
        "    self.linear_gate = tf.keras.layers.Dense(units=size)\n",
        "    self.linear = tf.keras.layers.Dense(units=size)\n",
        "    self.beta = self.add_weight(name='beta', shape=(1,), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def get_prunable_weights(self):\n",
        "    return self.trainable_weights\n",
        "\n",
        "  def call(self, x):\n",
        "    swish_gate = self.linear_gate(x) * tf.sigmoid(self.beta * self.linear_gate(x))\n",
        "    swi_glu = swish_gate * self.linear(x)\n",
        "    return swi_glu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting AttentionBlock\n",
        "added `get_prunable_weights` for future pruning"
      ],
      "metadata": {
        "id": "d7dh8_9HQmiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.rms = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "    self.rope_attention = RoPEAttentionLayer(config)\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "      [tf.keras.layers.Dense(units=config['d_model']),\n",
        "      SwiGLU(config['d_model']),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def get_prunable_weights(self):\n",
        "    return self.trainable_weights\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.rms(x)\n",
        "    x = x + self.rope_attention(x)\n",
        "\n",
        "    x = self.rms(x)\n",
        "    x = x + self.dense(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "2dTh-jeEDThm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BabyLLaMA Model\n",
        "**Difference**: converted the model from `subclassing` model to `Functional API`.  \n"
      ],
      "metadata": {
        "id": "sJcGv0kQybvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BabyLLaMAFunctionalModel(config):\n",
        "    inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "    # Embedding layer\n",
        "    embedded = tf.keras.layers.Embedding(input_dim=config['vocab_size'], output_dim=config['d_model'])(inputs)\n",
        "\n",
        "    # Attention blocks\n",
        "    x = embedded\n",
        "    for _ in range(config['n_layers']):\n",
        "        x = AttentionBlock(config)(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = tf.keras.layers.Dense(units=config['d_model'])(x)\n",
        "    x = SwiGLU(config['d_model'])(x)\n",
        "    logits = tf.keras.layers.Dense(units=config['vocab_size'])(x)\n",
        "\n",
        "    # 3. Create the model\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "uW8ahwlMyard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning (New Content)"
      ],
      "metadata": {
        "id": "D1rl0XH28q_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we load the original BabyLLaMA model (functinoal API version). `tf.keras.models.load_model()` resotres the model's architecture, weights, and training configuration.\n",
        "\n",
        "Note that we need to add `custom_objects` here. Our model includes custom-defined components (custom layers, custom activation functions, custom losses, custom metrics, etc) In this case, Keras won't be able to recognize them by default when we load the model, because these custom components are not part of the standard Keras library. This is where custom_objects comes into play.\n",
        "\n",
        "`custom_objects` is a dictionary that maps the names of our custom components to their respective Python objects (classes or functions), which tells Keras how to handle and instantiate these components when restoring the model.\n",
        "\n",
        "In our case, we have custom layers `AttentionBlock`, `SwiGLU`, `RMSNormLayer` and custom loss `custom_loss`"
      ],
      "metadata": {
        "id": "Le4rUUUSzJxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaed_model = BabyLLaMAFunctionalModel(MODEL_CONFIG)\n",
        "keras_file = os.path.join(model_dir, 'unpruned_model.keras')\n",
        "loaded_model = tf.keras.models.load_model(keras_file,\n",
        "            custom_objects={'AttentionBlock': AttentionBlock,\n",
        "                            'SwiGLU': SwiGLU,\n",
        "                            'RMSNormLayer': RMSNormLayer,\n",
        "                            'custom_loss': custom_loss})"
      ],
      "metadata": {
        "id": "Bqnjy3zpsuZ7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We inpsect the model's weights. Note that currently none of the weight is 0."
      ],
      "metadata": {
        "id": "aVzoDTSEzVcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_zeros = sum([np.sum(k.numpy() == 0) for k in model.trainable_weights])\n",
        "print(f\"Number of zero weights: {num_zeros}\")\n",
        "num_non_zeros = sum([np.sum(k.numpy() != 0) for k in model.trainable_weights])\n",
        "print(f\"Number of non zero weights: {num_non_zeros}\")\n",
        "print(f\"Zero rate: {num_zeros/(num_zeros+num_non_zeros)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km3WT9z5maCp",
        "outputId": "71489ddc-7db9-413d-c617-5afc6152efe9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of zero weights: 0\n",
            "Number of non zero weights: 2579142\n",
            "Zero rate: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the model performance again test data. The model accuray will be used as baseline."
      ],
      "metadata": {
        "id": "JvYqRgs703no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "loaded_model.evaluate(processed_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg-OLm6WtDfd",
        "outputId": "be4a84c4-304d-4d82-d485-11b9dd73ca7c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108/108 [==============================] - 15s 122ms/step - loss: 0.7568 - accuracy: 0.7673\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7568248510360718, 0.7673249244689941]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a pruned_model.\n",
        "\n",
        "`tfmot.sparsity.keras.prune_low_magnitude` is a function provided by TensorFlow Model Optimization Toolkit (TF-MOT) to apply weight pruning to Keras models and layers. The primary purpose of weight pruning is to set certain weights in the model to zero, thereby reducing the number of effective parameters and making the model smaller and faster. This is useful for deploying models on resource-constrained devices.\n",
        "\n",
        "When applied, prune_low_magnitude function modifies a model or layer to include the necessary operations for pruning and sets the initial configuration.\n",
        "\n",
        "\n",
        "In this example, we set the `initial_sparsity` to 0.5 (50% of weights will be 0) and the `final_sparsity` to 0.8 (80% of weights will be 0).\n",
        "\n",
        "\n",
        "`tfmot.sparsity.keras.PruningSummaries` is a Keras callback for adding pruning summaries to tensorboard."
      ],
      "metadata": {
        "id": "cMk0SI3i1Nht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# each epoch contains 1960 batches\n",
        "end_step = 1960 * MODEL_CONFIG['epochs']\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "pruned_model = prune_low_magnitude(loaded_model, **pruning_params)"
      ],
      "metadata": {
        "id": "khu7hhSltnl7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we recompile and train the pruned model\n",
        "\n",
        "`tfmot.sparsity.keras.UpdatePruningStep`\n",
        "* This is a callback that updates the pruning step during training. When we're training a pruned model, we need to inform the pruning algorithm about the current step so that it can decide whether to prune weights or not based on the pruning_schedule we've defined.\n",
        "* Essentially, every time an epoch finishes, this callback updates an internal counter that's used by the pruning algorithm to determine the current sparsity level based on our defined schedule (e.g., PolynomialDecay).\n",
        "* It's crucial to include this callback during training to ensure that pruning happens correctly.\n",
        "\n"
      ],
      "metadata": {
        "id": "ks7mpTDC2DGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model.compile(optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir, update_freq='epoch'),\n",
        "]\n",
        "\n",
        "pruned_model.fit(processed_train_dataset,\n",
        "                  batch_size=MODEL_CONFIG['batch_size'], epochs=MODEL_CONFIG['epochs'], validation_data=processed_validation_dataset,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "id": "AVZ2YmqZ1i9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cu9YcCrj4NSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"$log_dir\""
      ],
      "metadata": {
        "id": "2fa510Mh26Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check again to see the percentage of 0 weights. It's now 80%"
      ],
      "metadata": {
        "id": "dZsVCLxk3bax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_zeros = sum([np.sum(k.numpy() == 0) for k in pruned_model.trainable_weights])\n",
        "print(f\"Number of zero weights: {num_zeros}\")\n",
        "num_non_zeros = sum([np.sum(k.numpy() != 0) for k in pruned_model.trainable_weights])\n",
        "print(f\"Number of non zero weights: {num_non_zeros}\")\n",
        "print(f\"Prune rate: {num_zeros/(num_zeros+num_non_zeros)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DddKD-Z_1dUz",
        "outputId": "00361c5a-8f81-49dd-f4be-aff5f7512095"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of zero weights: 2063145\n",
            "Number of non zero weights: 515997\n",
            "Prune rate: 0.7999346294232733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the performance of the pruned model. We can see the performance impact is minimal."
      ],
      "metadata": {
        "id": "sSr7O_DQ3jSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model.evaluate(processed_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIvn31jZ3l8s",
        "outputId": "13bfdff6-2735-413e-e94d-68c13b9fd802"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108/108 [==============================] - 14s 126ms/step - loss: 0.8443 - accuracy: 0.7347\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8442904949188232, 0.7346643805503845]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the compression advantages of pruning, we must utilize both `tfmot.sparsity.keras.strip_pruning` and a conventional compression method like `gzip`.\n",
        "\n",
        "`strip_pruning` is crucial because it eliminates the `tf.Variable` elements that are only relevant during training, preventing them from inflating the model's size during inference.\n",
        "\n",
        "On the other hand, even after pruning, the serialized weight matrices retain their original size. The distinction now is that a significant portion of these weights are zeros. This increased redundancy introduced by pruning is what standard compression algorithms can capitalize on to achieve further model compression. That's why we also need `gzip` here."
      ],
      "metadata": {
        "id": "y-JFY0t84bEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's prepare a model for tensorflow"
      ],
      "metadata": {
        "id": "xUmVFDCD4s_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_tf_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.keras')\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "metadata": {
        "id": "vc6ngGTe4BGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's also prepare a model for `TFLite`"
      ],
      "metadata": {
        "id": "xUar2uKz4wzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_tf_model)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)\n"
      ],
      "metadata": {
        "id": "dBFyRNtGBRS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a helper function for gzipping the model"
      ],
      "metadata": {
        "id": "nnAMNG1A43Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "HfGWDXlxBUxQ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the mode sizes for\n",
        "* unpruned TensorFlow model\n",
        "* pruned TensorFlow model\n",
        "* prnued TFLite model"
      ],
      "metadata": {
        "id": "Z_Dp0_2E464t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wD1foxYBWOs",
        "outputId": "af74e61f-00bf-407b-b47b-02bbf47d108f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 9621758.00 bytes\n",
            "Size of gzipped pruned Keras model: 3113416.00 bytes\n",
            "Size of gzipped pruned TFlite model: 3141117.00 bytes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jg4xlzr_b1dV",
        "K9aY8Wmnb4sZ",
        "whLqzdDAQW7Q"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}