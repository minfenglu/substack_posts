{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "jg4xlzr_b1dV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "arSaj9CYUOoJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "K9aY8Wmnb4sZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtmNjv6IUVio"
      },
      "outputs": [],
      "source": [
        "# download tiny_shakespeare dataset\n",
        "dataset_dict = tfds.load(name='tiny_shakespeare')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict"
      ],
      "metadata": {
        "id": "1800cd9OiYoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vdo4hUBIVRs5"
      },
      "outputs": [],
      "source": [
        "# get train/validation/test data\n",
        "train_data = dataset_dict['train']\n",
        "validation_data = dataset_dict['validation']\n",
        "test_data = dataset_dict['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U9EpQqz9eV5h"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "each dataset contains 1 example of string type\n",
        "split the string into a sequence of Unicode code points\n",
        "'''\n",
        "train_dataset = train_data.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
        "validation_dataset = validation_data.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
        "test_dataset = test_data.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4WWL_UxIUp6i"
      },
      "outputs": [],
      "source": [
        "vocabulary = sorted(set(next(iter(train_dataset)).numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HEbxgbkQViTo"
      },
      "outputs": [],
      "source": [
        "ids_to_tokens = {id:token for id, token in enumerate(vocabulary)}\n",
        "tokens_to_ids = {token:id for id, token in enumerate(vocabulary)}\n",
        "\n",
        "keys_tensor = tf.constant(list(tokens_to_ids.keys()))\n",
        "vals_tensor = tf.constant(list(tokens_to_ids.values()))\n",
        "tokens_to_ids_loopup = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=-1\n",
        ")\n",
        "\n",
        "def decode(line):\n",
        "  return ''.join(ids_to_tokens[id].decode('UTF-8') for id in line)\n",
        "\n",
        "def tokenize(line):\n",
        "  return tokens_to_ids_loopup.lookup(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0OW6SprkYUS-"
      },
      "outputs": [],
      "source": [
        "# configuration to store model parameters\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"vocab_size\": len(vocabulary),\n",
        "    'batch_size': 32,\n",
        "    'context_window': 16,\n",
        "    'd_model': 128,\n",
        "    'epochs': 10,\n",
        "    'n_heads': 8,\n",
        "    'n_layers': 4,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DVFkZOY1lEzH"
      },
      "outputs": [],
      "source": [
        "def process_dataset(dataset, model_config, is_train=False):\n",
        "  dataset = dataset.map(lambda x: tokenize(x))\n",
        "  # shift the sequence by 1\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[1:]))\n",
        "  dataset = dataset.unbatch()\n",
        "  dataset = dataset.batch(model_config['context_window'], drop_remainder=True)\n",
        "  dataset = dataset.batch(model_config['batch_size'], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "processed_train_dataset = process_dataset(train_dataset, MODEL_CONFIG, True)\n",
        "processed_validation_dataset = process_dataset(validation_dataset, MODEL_CONFIG)\n",
        "processed_test_dataset = process_dataset(test_dataset, MODEL_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFUN6ERwRdgM"
      },
      "outputs": [],
      "source": [
        "# [OPTIONAL] store training results in google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_dir = '/content/drive/MyDrive/Colab Notebooks/llama'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# helper function to check for lingering tensorboards\n",
        "from tensorboard import notebook\n",
        "notebook.list()\n",
        "'''"
      ],
      "metadata": {
        "id": "K01ZbnnxyW64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BaseModel"
      ],
      "metadata": {
        "id": "whLqzdDAQW7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nympsQ-n1n8W"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = f\"{model_dir}/logs_base/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback_base = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "\n",
        "%tensorboard --logdir \"$log_dir\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model return logits rather than normalized probabilities\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "metadata": {
        "id": "eNHepr8jZuqs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxUpiuqMrmd1"
      },
      "outputs": [],
      "source": [
        "class BaseModel(tf.keras.Model):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    print(config)\n",
        "    self.config = config\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=config['vocab_size'], output_dim=config['d_model'])\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "        [tf.keras.layers.Dense(config['d_model']),\n",
        "         tf.keras.layers.ReLU(),\n",
        "         tf.keras.layers.Dense(config['vocab_size'])]\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    logits = self.dense(x)\n",
        "    return logits\n",
        "\n",
        "# store checkpoint\n",
        "checkpoint_callback_base = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{model_dir}/tmp/checkpoint_base',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch'  # save the model after each epoch\n",
        ")\n",
        "\n",
        "model = BaseModel(config=MODEL_CONFIG)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=custom_loss,\n",
        "              metrics=['accuracy'])\n",
        "model.fit(processed_train_dataset, validation_data=processed_validation_dataset, epochs=MODEL_CONFIG['epochs'], callbacks=[checkpoint_callback_base, tensorboard_callback_base])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kA1ihk35GgjP"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, config, sentence_count=5, max_new_tokens=50):\n",
        "    idx = tf.zeros([sentence_count, 1], dtype=tf.int64)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits  = model(idx[:, -config['context_window']:])\n",
        "        # get the distribution of the last token\n",
        "        p = logits[:, -1,:]\n",
        "        # use the distribution p to sample the next token\n",
        "        idx_next = tf.random.categorical(p, num_samples=1, dtype=tf.int64)\n",
        "        idx = tf.concat([idx, idx_next], axis=-1)\n",
        "    return [decode(line) for line in idx.numpy()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = generate_text(model, MODEL_CONFIG)\n",
        "texts"
      ],
      "metadata": {
        "id": "D3DeYsY_Zyfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSNorm"
      ],
      "metadata": {
        "id": "cmVnyGAZQaX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNormLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "        super().__init__()\n",
        "        self.scale = self.add_weight(\"scale\", shape=layer_shape, initializer=\"ones\", trainable=True)\n",
        "        self.eps = eps\n",
        "        if bias:\n",
        "            self.bias = self.add_weight(\"bias\", shape=layer_shape, initializer=\"zeros\", trainable=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def call(self, x):\n",
        "        # Frobenius norm\n",
        "        fro_norm = tf.norm(x, ord='fro', axis=[1, 2]) * tf.math.pow(tf.cast(tf.reduce_prod(tf.shape(x[0])), tf.float32), -0.5)\n",
        "        normalized = x / (tf.expand_dims(tf.expand_dims(fro_norm, -1), -1) + self.eps)\n",
        "        scaled = tf.expand_dims(self.scale[:x.shape[1], :], 0) * normalized\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return scaled + self.bias\n",
        "        else:\n",
        "            return scaled"
      ],
      "metadata": {
        "id": "6B7zqk765449"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ElW5vo22g4YZ"
      },
      "outputs": [],
      "source": [
        "# test RMSNormLayer\n",
        "\n",
        "# use different configuration values to make sure\n",
        "# the layer is able to handle all\n",
        "config = {\n",
        "    'batch_size': 5,\n",
        "    'context_window': 11,\n",
        "    'd_model': 13,\n",
        "}\n",
        "\n",
        "def test_output_shape():\n",
        "  batch = tf.random.normal((config['batch_size'], config['context_window'], config['d_model']))\n",
        "  rms_layer = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "  output = rms_layer(batch)\n",
        "  tf.debugging.assert_shapes([(output, (config['batch_size'], config['context_window'], config['d_model']))])\n",
        "test_output_shape()\n",
        "\n",
        "def test_layer_initialization():\n",
        "  rms_layer = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "  tf.debugging.assert_near(tf.reduce_mean(rms_layer.scale), 1.0, atol=1e-5)\n",
        "test_layer_initialization()\n",
        "\n",
        "def test_output_variance():\n",
        "    batch = tf.random.normal((config['batch_size'], config['context_window'], config['d_model']))\n",
        "    rms_layer = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "    output = rms_layer(batch)\n",
        "    variance = tf.math.reduce_variance(output)\n",
        "    tf.debugging.assert_near(variance, 1.0, atol=1e-1)\n",
        "test_output_variance()\n",
        "\n",
        "def test_frobenius_norm_calculation():\n",
        "    batch = tf.random.normal((config['batch_size'], config['context_window'], config['d_model']))\n",
        "    rms = tf.norm(batch, axis=(1, 2)) * (tf.cast(tf.size(batch[0]), tf.float32) ** -0.5)\n",
        "    assert rms.shape == (config['batch_size'],), \"RMS shape is incorrect\"\n",
        "test_frobenius_norm_calculation()\n",
        "\n",
        "def test_tf_norm_equivalence():\n",
        "    tf.debugging.assert_near(\n",
        "        tf.norm(tf.range(5, dtype=tf.float32)),\n",
        "        tf.math.sqrt(tf.reduce_sum(tf.square(tf.range(5, dtype=tf.float32))))\n",
        "    )\n",
        "test_tf_norm_equivalence()\n",
        "\n",
        "def test_normalized_tensor_norm():\n",
        "    rms_single = tf.norm(tf.range(5, dtype=tf.float32)) * (tf.cast(tf.size(tf.range(5, dtype=tf.float32)), tf.float32) ** -0.5)\n",
        "    tf.debugging.assert_near(\n",
        "        tf.norm(tf.range(5, dtype=tf.float32) / rms_single),\n",
        "        tf.math.sqrt(tf.constant(5, dtype=tf.float32))\n",
        "    )\n",
        "test_normalized_tensor_norm()\n",
        "\n",
        "def test_ff_rms_calculation():\n",
        "    batch = tf.random.normal((config['batch_size'], config['context_window'], config['d_model']))\n",
        "    ff_rms = tf.norm(batch, axis=(1, 2)) * (tf.cast(tf.math.reduce_prod(tf.shape(batch)[1:]), tf.float32) ** -0.5)\n",
        "    assert ff_rms.shape == (config['batch_size'],), \"FF RMS shape is incorrect\"\n",
        "test_ff_rms_calculation()\n",
        "\n",
        "def test_per_batch_item_normalization():\n",
        "    batch = tf.random.normal((config['batch_size'], config['context_window'], config['d_model']))\n",
        "    ff_rms = tf.norm(batch, axis=(1, 2)) * (tf.cast(tf.math.reduce_prod(tf.shape(batch)[1:]), tf.float32) ** -0.5)\n",
        "\n",
        "    ffx = tf.zeros_like(batch)\n",
        "    for i in range(tf.shape(batch)[0]):\n",
        "        ffx = tf.tensor_scatter_nd_update(ffx, [[i]], [batch[i] / ff_rms[i]])\n",
        "\n",
        "    tf.debugging.assert_near(\n",
        "        tf.square(tf.norm(ffx, axis=(1, 2))),\n",
        "        tf.constant(143, dtype=tf.float32)\n",
        "    )\n",
        "test_per_batch_item_normalization()\n",
        "\n",
        "def test_rmsnorm_layer_output():\n",
        "    batch = tf.random.normal((config['batch_size'], config['context_window'], config['d_model']))\n",
        "    rms_layer = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "    g = rms_layer(batch)\n",
        "\n",
        "    ff_rms = tf.norm(batch, axis=(1, 2)) * (tf.cast(tf.math.reduce_prod(tf.shape(batch)[1:]), tf.float32) ** -0.5)\n",
        "    ffx = tf.zeros_like(batch)\n",
        "    for i in range(tf.shape(batch)[0]):\n",
        "        ffx = tf.tensor_scatter_nd_update(ffx, [[i]], [batch[i] / ff_rms[i]])\n",
        "\n",
        "    tf.debugging.assert_near(ffx, g)\n",
        "test_rmsnorm_layer_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgoywkAUn0_4"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = f\"{model_dir}/rms_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback_rms = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "\n",
        "%tensorboard --logdir \"$log_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KKkwINrm6_5"
      },
      "outputs": [],
      "source": [
        "class RMSModel(tf.keras.Model):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=config['vocab_size'], output_dim=config['d_model'])\n",
        "    self.rms = RMSNormLayer((config['context_window'], config['d_model'])) # new layer\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "        [tf.keras.layers.Dense(units=config['d_model']),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(units=config['vocab_size'])]\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.rms(x) # new layer\n",
        "    logits = self.dense(x)\n",
        "    return logits\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint_callback_rms = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{model_dir}/tmp/checkpoint_rms',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch'  # save the model after each epoch\n",
        ")\n",
        "\n",
        "model = RMSModel(config=MODEL_CONFIG)\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "model.fit(processed_train_dataset,  validation_data=processed_validation_dataset, epochs=MODEL_CONFIG['epochs'], callbacks=[tensorboard_callback_rms, checkpoint_callback_rms])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0Q9cBvZpNu6"
      },
      "outputs": [],
      "source": [
        "texts = generate_text(model, MODEL_CONFIG)\n",
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoPE"
      ],
      "metadata": {
        "id": "93Lz6Q3TQeYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "  matrix = tf.zeros((context_window, embedding_dim, embedding_dim))\n",
        "\n",
        "  for position in range(context_window):\n",
        "    for i in range(embedding_dim // 2):\n",
        "      theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
        "      m_theta = position * theta\n",
        "\n",
        "      matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i, 2 * i]], [tf.math.cos(m_theta)])\n",
        "      matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i, 2 * i + 1]], [-tf.math.sin(m_theta)])\n",
        "      matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i + 1, 2 * i]], [tf.math.sin(m_theta)])\n",
        "      matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i + 1, 2 * i + 1]], [tf.math.cos(m_theta)])\n",
        "\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "IG4MfnVE9Jw8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d_iross-w8Cs"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  'd_model': 128,\n",
        "  'context_window': 16,\n",
        "}\n",
        "\n",
        "def test_rotary_matrix_shape():\n",
        "  matrix = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "  tf.debugging.assert_shapes([(matrix, (config['context_window'], config['d_model'], config['d_model']))])\n",
        "test_rotary_matrix_shape()\n",
        "\n",
        "def test_rotary_matrix_values():\n",
        "  matrix = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "  for position in range(config['context_window']):\n",
        "    for i in range(config['d_model'] // 2):\n",
        "      theta = 10000. ** (-2. * (i - 1) / config['d_model'])\n",
        "      m_theta = position * theta\n",
        "      assert matrix[position, 2 * i, 2 * i] == tf.math.cos(m_theta)\n",
        "      assert matrix[position, 2 * i, 2 * i + 1] == -tf.math.sin(m_theta)\n",
        "      assert matrix[position, 2 * i + 1, 2 * i] == tf.math.sin(m_theta)\n",
        "      assert matrix[position, 2 * i + 1, 2 * i + 1] == tf.math.cos(m_theta)\n",
        "test_rotary_matrix_values()\n",
        "\n",
        "def test_rotary_matrix_multiplication_properties():\n",
        "  rotary_matrix = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "  # Random tensors for x and y\n",
        "  x = tf.random.normal((config['d_model'],))\n",
        "  y = tf.random.normal((config['d_model'],))\n",
        "\n",
        "  m = 3\n",
        "  n = 13\n",
        "\n",
        "  # Matrix-vector multiplication\n",
        "  x_m = tf.linalg.matvec(rotary_matrix[m, :, :], x)\n",
        "  x_n = tf.linalg.matvec(rotary_matrix[n, :, :], y)\n",
        "\n",
        "  tf.debugging.assert_near(\n",
        "    tf.tensordot(x_m, x_n, axes=1),\n",
        "    tf.tensordot(x, tf.linalg.matvec(rotary_matrix[n-m, :, :], y), axes=1)\n",
        "  )\n",
        "test_rotary_matrix_multiplication_properties()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "CUzGCzpcbIUg"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "  mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_square_subsequent_mask(5)"
      ],
      "metadata": {
        "id": "6obFbvSJw8i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Q_PubLCWxIop"
      },
      "outputs": [],
      "source": [
        "class RoPEAttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.w_q = tf.keras.layers.Dense(config['d_model'], use_bias=False)\n",
        "    self.w_k = tf.keras.layers.Dense(config['d_model'], use_bias=False)\n",
        "    self.w_v = tf.keras.layers.Dense(config['d_model'], use_bias=False)\n",
        "\n",
        "    self.multihead = tf.keras.layers.MultiHeadAttention(config['n_heads'], config['d_model'], dropout=0.1)\n",
        "\n",
        "    self.rotary_matrix = self.get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "\n",
        "  def get_rotary_matrix(self, context_window, embedding_dim):\n",
        "    matrix = tf.zeros((context_window, embedding_dim, embedding_dim))\n",
        "\n",
        "    for position in range(context_window):\n",
        "      for i in range(embedding_dim // 2):\n",
        "        theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
        "        m_theta = position * theta\n",
        "\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i, 2 * i]], [tf.math.cos(m_theta)])\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i, 2 * i + 1]], [-tf.math.sin(m_theta)])\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i + 1, 2 * i]], [tf.math.sin(m_theta)])\n",
        "        matrix = tf.tensor_scatter_nd_update(matrix, [[position, 2 * i + 1, 2 * i + 1]], [tf.math.cos(m_theta)])\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  def call(self, x):\n",
        "    _, context_window, _ = x.shape\n",
        "    q = self.w_q(x)\n",
        "    k = self.w_k(x)\n",
        "    v = self.w_v(x)\n",
        "    q_out = tf.linalg.matmul(tf.transpose(q, perm=[1, 0, 2]), self.rotary_matrix[:context_window, ...])\n",
        "    q_out = tf.transpose(q_out, perm=[1, 0, 2])\n",
        "    k_out = tf.linalg.matmul(tf.transpose(k, perm=[1, 0, 2]), self.rotary_matrix[:context_window, ...])\n",
        "    k_out = tf.transpose(k_out, perm=[1, 0, 2])\n",
        "    v_out = tf.linalg.matmul(tf.transpose(v, perm=[1, 0, 2]), self.rotary_matrix[:context_window, ...])\n",
        "    v_out = tf.transpose(v_out, perm=[1, 0, 2])\n",
        "\n",
        "    activations = self.multihead(\n",
        "        q_out, k_out, v_out,\n",
        "        attention_mask=generate_square_subsequent_mask(context_window),\n",
        "        return_attention_scores=False,\n",
        "        use_causal_mask=True\n",
        "    )\n",
        "    return activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "338nzHddIz-j"
      },
      "outputs": [],
      "source": [
        "#%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = f\"{model_dir}/logs_rope/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback_rope = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "\n",
        "%tensorboard --logdir \"$log_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtHGlYv-FFCJ"
      },
      "outputs": [],
      "source": [
        "class RoPEModel(tf.keras.Model):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=config['vocab_size'], output_dim=config['d_model'])\n",
        "    self.rms = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "    self.rope_attention = RoPEAttentionLayer(config)\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "        [tf.keras.layers.Dense(units=config['d_model']),\n",
        "         tf.keras.layers.ReLU(),\n",
        "        ],\n",
        "\n",
        "    )\n",
        "    self.dense_last = tf.keras.layers.Dense(units=config['vocab_size'])\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = self.rms(x)\n",
        "    x = x + self.rope_attention(x)\n",
        "\n",
        "    x = self.rms(x)\n",
        "    x = x + self.dense(x)\n",
        "    logits = self.dense_last(x)\n",
        "    return logits\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint_callback_rope = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{model_dir}/tmp/checkpoint_rope',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch'  # save the model after each epoch\n",
        ")\n",
        "\n",
        "model = RoPEModel(config=MODEL_CONFIG)\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "model.fit(processed_train_dataset, validation_data=processed_validation_dataset, epochs=MODEL_CONFIG['epochs'], callbacks=[tensorboard_callback_rope, checkpoint_callback_rope])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = generate_text(model, MODEL_CONFIG)\n",
        "texts"
      ],
      "metadata": {
        "id": "KDUgogpGaRvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SwiGLU"
      ],
      "metadata": {
        "id": "IXYb286OQiqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "fwmFcNYiafck"
      },
      "outputs": [],
      "source": [
        "class SwiGLU(tf.keras.layers.Layer):\n",
        "  def __init__(self, size):\n",
        "    super().__init__()\n",
        "    self.linear_gate = tf.keras.layers.Dense(units=size)\n",
        "    self.linear = tf.keras.layers.Dense(units=size)\n",
        "    self.beta = self.add_weight(name='beta', shape=(1,), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    swish_gate = self.linear_gate(x) * tf.sigmoid(self.beta * self.linear_gate(x))\n",
        "    swi_glu = swish_gate * self.linear(x)\n",
        "    return swi_glu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = f\"{model_dir}/logs_swiglu/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback_swiglu = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "\n",
        "%tensorboard --logdir \"$log_dir\""
      ],
      "metadata": {
        "id": "_lAOxkfyvJGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLUModel(tf.keras.Model):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=config['vocab_size'], output_dim=config['d_model'])\n",
        "    self.rms = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "    self.rope_attention = RoPEAttentionLayer(config)\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "        [tf.keras.layers.Dense(units=config['d_model']),\n",
        "         SwiGLU(config['d_model']),\n",
        "        ],\n",
        "\n",
        "    )\n",
        "    self.dense_last = tf.keras.layers.Dense(units=config['vocab_size'])\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = self.rms(x)\n",
        "    x = x + self.rope_attention(x)\n",
        "\n",
        "    x = self.rms(x)\n",
        "    x = x + self.dense(x)\n",
        "    logits = self.dense_last(x)\n",
        "    return logits\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint_callback_swiglu = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{model_dir}/tmp/checkpoint_swiglu',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch'  # save the model after each epoch\n",
        ")\n",
        "\n",
        "model = SwiGLUModel(config=MODEL_CONFIG)\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "model.fit(processed_train_dataset, validation_data=processed_validation_dataset, epochs=MODEL_CONFIG['epochs'], callbacks=[tensorboard_callback_swiglu, checkpoint_callback_swiglu])"
      ],
      "metadata": {
        "id": "p84Q5uCauwJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb1pMhEX8bMI"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = f\"{model_dir}/logs_baby/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback_baby = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "\n",
        "%tensorboard --logdir \"$log_dir\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting Evenrything Together: BabyLLaMA"
      ],
      "metadata": {
        "id": "d7dh8_9HQmiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnwjLB68bmsx"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.rms = RMSNormLayer((config['context_window'], config['d_model']))\n",
        "    self.rope_attention = RoPEAttentionLayer(config)\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "      [tf.keras.layers.Dense(units=config['d_model']),\n",
        "      SwiGLU(config['d_model']),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.rms(x)\n",
        "    x = x + self.rope_attention(x)\n",
        "\n",
        "    x = self.rms(x)\n",
        "    x = x + self.dense(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BabyLLaMA(tf.keras.Model):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=config['vocab_size'], output_dim=config['d_model'])\n",
        "    self.attention_blocks = tf.keras.models.Sequential(\n",
        "        [\n",
        "          AttentionBlock(config) for _ in range(config['n_layers'])\n",
        "        ]\n",
        "    )\n",
        "    self.dense = tf.keras.models.Sequential(\n",
        "      [tf.keras.layers.Dense(units=config['d_model']),\n",
        "       SwiGLU(config['d_model']),\n",
        "       tf.keras.layers.Dense(units=config['vocab_size']),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.attention_blocks(x)\n",
        "    logits = self.dense(x)\n",
        "    return logits\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint_callback_baby = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{model_dir}/tmp/checkpoint_baby',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch'  # save the model after each epoch\n",
        ")\n",
        "\n",
        "model = BabyLLaMA(config=MODEL_CONFIG)\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "model.fit(processed_train_dataset, validation_data=processed_validation_dataset, epochs=MODEL_CONFIG['epochs'], callbacks=[tensorboard_callback_baby, checkpoint_callback_baby])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUXWdXu3DMGL"
      },
      "outputs": [],
      "source": [
        "texts = generate_text(model, MODEL_CONFIG, 5, 500)\n",
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import datetime\n",
        "log_dir = f\"{model_dir}/logs_final/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback_final = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "\n",
        "%tensorboard --logdir \"$log_dir\""
      ],
      "metadata": {
        "id": "4euC6xgZNCaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_learning_rate = 1e-3\n",
        "decay_steps = 300\n",
        "end_learning_rate = 1e-5\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate, decay_steps, alpha=end_learning_rate)\n",
        "\n",
        "custom_optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=lr_schedule,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.95,\n",
        "    epsilon=1e-9,\n",
        "    weight_decay=0.1\n",
        ")\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint_callback_final = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{model_dir}/tmp/checkpoint_final',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch'  # save the model after each epoch\n",
        ")\n",
        "\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "model.fit(processed_train_dataset, validation_data=processed_validation_dataset, epochs=MODEL_CONFIG['epochs'], callbacks=[tensorboard_callback_final, checkpoint_callback_final])\n"
      ],
      "metadata": {
        "id": "fuC94ZswMqAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(f'{model_dir}/tmp/checkpoint_final')"
      ],
      "metadata": {
        "id": "33YkhUfiO9Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = generate_text(model, MODEL_CONFIG, 5, 500)\n",
        "texts"
      ],
      "metadata": {
        "id": "avLMyQSEQL3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}